{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9b6bdd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load pretrained CLIP\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").cuda()\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "56f607f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MIMIC-CXR dataset\n",
    "train_dataset = load_dataset(\"itsanmolgupta/mimic-cxr-dataset\", split=\"train[:90%]\")  \n",
    "val_datset = load_dataset(\"itsanmolgupta/mimic-cxr-dataset\", split=\"train[:10%]\")\n",
    "train_dataset = train_dataset.filter(lambda x: x['impression'] is not None)\n",
    "val_dataset = val_datset.filter(lambda x: x['impression'] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0490b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'findings', 'impression'],\n",
       "    num_rows: 27561\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "68fb30de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'findings', 'impression'],\n",
       "    num_rows: 3062\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7cb235ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512>,\n",
       " 'findings': 'Lung volumes remain low. There are innumerable bilateral scattered small pulmonary nodules which are better demonstrated on recent CT. Mild pulmonary vascular congestion is stable. The cardiomediastinal silhouette and hilar contours are unchanged. Small pleural effusion in the right middle fissure is new. There is no new focal opacity to suggest pneumonia. There is no pneumothorax. ',\n",
       " 'impression': 'Low lung volumes and mild pulmonary vascular congestion is unchanged. New small right fissural pleural effusion. No new focal opacities to suggest pneumonia.'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc46b322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "daf52b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess(examples):\n",
    "    images = [img.convert(\"RGB\") for img in examples[\"image\"]]\n",
    "    texts = [text if text else \"\" for text in examples[\"impression\"]]\n",
    "    inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    return inputs\n",
    "\n",
    "# Custom PyTorch dataset wrapper\n",
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        image = example[\"image\"].convert(\"RGB\")\n",
    "        text = example[\"impression\"]\n",
    "        return {\"image\": image, \"text\": text}\n",
    "\n",
    "# Collate function using CLIPProcessor\n",
    "def collate_fn(batch):\n",
    "    images = [b[\"image\"] for b in batch]\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    return processor(text=texts, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f151971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "# from torch.utils.data import DataLoader\n",
    "# import time\n",
    "# from datetime import timedelta\n",
    "# import numpy as np\n",
    "\n",
    "# def do_train(model, train_dl, optimizer, lr_scheduler, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     total_batches = 0\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # Track metrics for reporting\n",
    "#     running_loss = 0\n",
    "#     log_interval = 100\n",
    "    \n",
    "#     for bid, batch in enumerate(train_dl):\n",
    "#         batch_start = time.time()\n",
    "        \n",
    "#         # Move batch to device\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(**batch, return_loss=True)\n",
    "#         loss = outputs.loss\n",
    "        \n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         lr_scheduler.step()\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Accumulate loss (use float value to avoid GPU memory buildup)\n",
    "#         batch_loss = loss.item()\n",
    "#         total_loss += batch_loss\n",
    "#         total_batches += 1\n",
    "#         running_loss += batch_loss\n",
    "        \n",
    "#         # Logging\n",
    "#         if bid % log_interval == 0:\n",
    "#             if bid > 0:\n",
    "#                 avg_running_loss = running_loss / log_interval\n",
    "#                 running_loss = 0\n",
    "#                 batch_time = time.time() - batch_start\n",
    "#                 eta = batch_time * (len(train_dl) - bid)\n",
    "                \n",
    "#                 print(f\"Batch {bid}/{len(train_dl)} | \" \n",
    "#                       f\"Loss: {avg_running_loss:.4f} | \"\n",
    "#                       f\"LR: {lr_scheduler.get_last_lr()[0]:.6f} | \"\n",
    "#                       f\"ETA: {str(timedelta(seconds=int(eta)))}\")\n",
    "#             else:\n",
    "#                 print(f\"Starting training on {len(train_dl)} batches...\")\n",
    "    \n",
    "#     # Calculate average loss over all batches\n",
    "#     avg_loss = total_loss / total_batches if total_batches > 0 else 0\n",
    "#     total_time = time.time() - start_time\n",
    "    \n",
    "#     print(f\"Training complete: {total_batches} batches in {str(timedelta(seconds=int(total_time)))}\")\n",
    "#     print(f\"Average batch time: {total_time/total_batches:.2f}s | Average loss: {avg_loss:.4f}\")\n",
    "    \n",
    "#     return avg_loss, total_time\n",
    "\n",
    "\n",
    "# def do_eval(model, eval_dl, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "#     total_examples = 0\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # Make sure we have data to validate on\n",
    "#     if len(eval_dl) == 0:\n",
    "#         print(\"Warning: Empty validation dataloader!\")\n",
    "#         return 0, 0, 0\n",
    "    \n",
    "#     print(f\"Starting validation on {len(eval_dl)} batches...\")\n",
    "    \n",
    "#     for bid, batch in enumerate(eval_dl):\n",
    "#         # Move batch to device\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "#         # Forward pass without gradients\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**batch, return_loss=True)\n",
    "        \n",
    "#         # Calculate loss and accumulate\n",
    "#         loss = outputs.loss\n",
    "#         total_loss += loss.item()\n",
    "        \n",
    "#         # Calculate accuracy: in CLIP, the diagonal elements should have the highest score\n",
    "#         logits_per_image = outputs.logits_per_image\n",
    "#         batch_size = logits_per_image.size(0)\n",
    "        \n",
    "#         if batch_size == 0:\n",
    "#             print(f\"Warning: Batch {bid} has size 0\")\n",
    "#             continue\n",
    "            \n",
    "#         # The expected labels in CLIP are the diagonal (matching text-image pairs)\n",
    "#         labels = torch.arange(batch_size).to(device)\n",
    "#         predictions = torch.argmax(logits_per_image, dim=1)\n",
    "        \n",
    "#         # Count correct predictions\n",
    "#         batch_correct = (predictions == labels).sum().item()\n",
    "#         correct += batch_correct\n",
    "#         total_examples += batch_size\n",
    "        \n",
    "#         # Log progress\n",
    "#         if (bid + 1) % 10 == 0 or (bid + 1) == len(eval_dl):\n",
    "#             print(f\"Validated {bid+1}/{len(eval_dl)} batches | \"\n",
    "#                   f\"Current Accuracy: {100 * correct / total_examples:.2f}%\")\n",
    "    \n",
    "#     # Calculate averages\n",
    "#     avg_loss = total_loss / len(eval_dl) if len(eval_dl) > 0 else 0\n",
    "#     accuracy = correct / total_examples if total_examples > 0 else 0\n",
    "#     total_time = time.time() - start_time\n",
    "    \n",
    "#     print(f\"Validation complete: {total_examples} examples in {str(timedelta(seconds=int(total_time)))}\")\n",
    "#     print(f\"Validation Loss: {avg_loss:.4f} | Accuracy: {100 * accuracy:.2f}%\")\n",
    "    \n",
    "#     return avg_loss, accuracy, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "43b9a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import time\n",
    "# from torch.utils.data import DataLoader\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# def do_train(model, train_loader, optimizer, epoch, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     for batch in train_loader:\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "#         outputs = model(**batch, return_loss=True)\n",
    "#         loss = outputs.loss\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     train_time = time.time() - start_time\n",
    "#     avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "#     print(f\"âœ… Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f} - Time: {train_time:.2f}s\")\n",
    "#     return avg_train_loss, train_time\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def do_eval(model, val_loader, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     for batch in val_loader:\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "#         outputs = model(**batch, return_loss=True)\n",
    "#         loss = outputs.loss\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Cosine similarity for prediction\n",
    "#         image_embeds = model.get_image_features(pixel_values=batch['pixel_values'])\n",
    "#         text_embeds = model.get_text_features(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "\n",
    "#         # Normalize\n",
    "#         image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "#         text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#         logits = (image_embeds @ text_embeds.T)  # [B, B]\n",
    "#         preds = torch.argmax(logits, dim=1)\n",
    "#         labels = torch.arange(len(preds)).to(device)\n",
    "\n",
    "#         all_preds.extend(preds.cpu().tolist())\n",
    "#         all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "#     val_time = time.time() - start_time\n",
    "#     avg_val_loss = total_loss / len(val_loader)\n",
    "#     val_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "#     print(f\"ðŸ“Š Validation Loss: {avg_val_loss:.4f} - Val_Accuracy: {val_acc*100:.2f}% - Time: {val_time:.2f}s\")\n",
    "#     return avg_val_loss, val_acc, val_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4855519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f22efa20",
   "metadata": {},
   "source": [
    "## QWEN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ae492ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen suggestion:\n",
    "def do_train(model, train_loader, optimizer, epoch, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        image_embeds = model.get_image_features(pixel_values=batch['pixel_values'])\n",
    "        text_embeds = model.get_text_features(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "\n",
    "        # Normalize embeddings\n",
    "        image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "        text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute logits and loss\n",
    "        logit_scale = model.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_embeds @ text_embeds.T\n",
    "        logits_per_text = logits_per_image.T\n",
    "        labels = torch.arange(len(logits_per_image)).to(device)\n",
    "        loss = (F.cross_entropy(logits_per_image, labels) + F.cross_entropy(logits_per_text, labels)) / 2\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"âœ… Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f} - Time: {train_time:.2f}s\")\n",
    "    return avg_train_loss, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "23a0cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def do_eval(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    all_preds=[]\n",
    "\n",
    "    all_preds_top1 = []\n",
    "    all_preds_top5 = []\n",
    "    all_labels = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in val_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        image_embeds = model.get_image_features(pixel_values=batch['pixel_values'])\n",
    "        text_embeds = model.get_text_features(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "\n",
    "        # Normalize embeddings\n",
    "        image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "        text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute logits with temperature scaling\n",
    "        logit_scale = model.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_embeds @ text_embeds.T\n",
    "        logits_per_text = logits_per_image.T\n",
    "\n",
    "        # Contrastive loss\n",
    "        labels = torch.arange(len(logits_per_image)).to(device)\n",
    "        loss = (F.cross_entropy(logits_per_image, labels) + F.cross_entropy(logits_per_text, labels)) / 2\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Top-1 and Top-5 predictions\n",
    "        preds_top1 = torch.argmax(logits_per_image, dim=1)\n",
    "        preds_top5 = torch.topk(logits_per_image, k=5, dim=1).indices\n",
    "\n",
    "        preds = torch.argmax(logits_per_image, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        all_preds_top1.extend(preds_top1.cpu().tolist())\n",
    "        all_preds_top5.extend(preds_top5.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    val_time = time.time() - start_time\n",
    "    avg_val_loss = total_loss / len(val_loader)\n",
    "\n",
    "    # Calculate Top-1 and Top-5 accuracy\n",
    "    val_acc = accuracy_score(all_labels, all_preds)   \n",
    "    val_acc_top1 = accuracy_score(all_labels, all_preds_top1)\n",
    "    val_acc_top5 = sum(label in pred for label, pred in zip(all_labels, all_preds_top5)) / len(all_labels)\n",
    "    print(f\"ðŸ“Š Validation Loss: {avg_val_loss:.4f} | val_Acc: {val_acc*100:.4f}% |Top-1 Acc: {val_acc_top1*100:.2f}% | Top-5 Acc: {val_acc_top5*100:.2f}% | Time: {val_time:.2f}s\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"val_loss\": avg_val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_acc_top1\": val_acc_top1,\n",
    "        \"val_acc_top5\": val_acc_top5,\n",
    "        \"val_time\": val_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f15dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_ds = CLIPDataset(train_dataset)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "# Validation DataLoader\n",
    "val_dataset = CLIPDataset(val_dataset)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d27c24ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, ReduceLROnPlateau\n",
    "# from transformers import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e44d77",
   "metadata": {},
   "source": [
    "## CLAUDE SUGGESTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f2e82d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CLAUDE SUGGESTION:\n",
    "\n",
    "\n",
    "\n",
    "# def do_train(model, train_loader, optimizer, epoch, device, scheduler=None, max_grad_norm=1.0):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     for batch_idx, batch in enumerate(train_loader):\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(**batch, return_loss=True)\n",
    "#         loss = outputs.loss\n",
    "        \n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # Gradient clipping for stability\n",
    "#         if max_grad_norm > 0:\n",
    "#             clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "        \n",
    "#         # Report progress for long epochs\n",
    "#         if batch_idx % 50 == 0:\n",
    "#             print(f\"  Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "    \n",
    "#     # Step the scheduler if provided\n",
    "#     if scheduler is not None:\n",
    "#         scheduler.step()\n",
    "    \n",
    "#     train_time = time.time() - start_time\n",
    "#     avg_train_loss = total_loss / len(train_loader)\n",
    "    \n",
    "#     return avg_train_loss, train_time\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def do_eval(model, val_loader, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     for batch in val_loader:\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "#         # Get loss\n",
    "#         outputs = model(**batch, return_loss=True)\n",
    "#         loss = outputs.loss\n",
    "#         total_loss += loss.item()\n",
    "        \n",
    "#         # Get image and text features\n",
    "#         image_embeds = model.get_image_features(pixel_values=batch['pixel_values'])\n",
    "#         text_embeds = model.get_text_features(\n",
    "#             input_ids=batch['input_ids'], \n",
    "#             attention_mask=batch['attention_mask']\n",
    "#         )\n",
    "        \n",
    "#         # Normalize\n",
    "#         image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "#         text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "#         # Compute similarity\n",
    "#         logits = torch.matmul(image_embeds, text_embeds.T) * model.logit_scale.exp()\n",
    "        \n",
    "#         # Prediction (diagonal elements should be highest)\n",
    "#         preds = torch.argmax(logits, dim=1)\n",
    "#         labels = torch.arange(len(preds)).to(device)\n",
    "        \n",
    "#         all_preds.extend(preds.cpu().tolist())\n",
    "#         all_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "#     val_time = time.time() - start_time\n",
    "#     avg_val_loss = total_loss / len(val_loader)\n",
    "#     val_acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "#     return avg_val_loss, val_acc, val_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8e5aa096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main training function with scheduler\n",
    "# def train_clip_model(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     num_epochs=10,\n",
    "#     learning_rate=2e-5,\n",
    "#     weight_decay=0.01,\n",
    "#     scheduler_type=\"cosine\",  # Options: \"cosine\", \"linear\", \"plateau\", \"none\"\n",
    "#     warmup_epochs=1,\n",
    "#     checkpoint_dir=\"checkpoints\",\n",
    "#     device=None\n",
    "# ):\n",
    "#     if device is None:\n",
    "#         device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "#     # Move model to device\n",
    "#     model = model.to(device)\n",
    "    \n",
    "#     # Create optimizer - AdamW is typically used for transformers\n",
    "#     optimizer = torch.optim.AdamW(\n",
    "#         model.parameters(),\n",
    "#         lr=learning_rate,\n",
    "#         weight_decay=weight_decay\n",
    "#     )\n",
    "    \n",
    "#     # Create scheduler based on type\n",
    "#     scheduler = None\n",
    "#     if scheduler_type == \"cosine\":\n",
    "#         # Cosine decay from initial lr to 0\n",
    "#         scheduler = CosineAnnealingLR(\n",
    "#             optimizer, \n",
    "#             T_max=num_epochs - warmup_epochs,\n",
    "#             eta_min=1e-6\n",
    "#         )\n",
    "        \n",
    "#         # Optional: Warmup scheduler for first few epochs\n",
    "#         if warmup_epochs > 0:\n",
    "#             warmup_scheduler = LinearLR(\n",
    "#                 optimizer, \n",
    "#                 start_factor=0.1, \n",
    "#                 end_factor=1.0, \n",
    "#                 total_iters=warmup_epochs\n",
    "#             )\n",
    "            \n",
    "#     elif scheduler_type == \"linear\":\n",
    "#         # Linear decay from initial lr to 0\n",
    "#         scheduler = LinearLR(\n",
    "#             optimizer,\n",
    "#             start_factor=1.0,\n",
    "#             end_factor=0.1,\n",
    "#             total_iters=num_epochs\n",
    "#         )\n",
    "        \n",
    "#     elif scheduler_type == \"plateau\":\n",
    "#         # Reduce LR when validation loss plateaus\n",
    "#         scheduler = ReduceLROnPlateau(\n",
    "#             optimizer,\n",
    "#             mode='min',\n",
    "#             factor=0.5,\n",
    "#             patience=2,\n",
    "#             verbose=True\n",
    "#         )\n",
    "    \n",
    "#     # Create checkpoint directory if it doesn't exist\n",
    "#     os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "#     best_val_acc = 0\n",
    "#     best_model_path = None\n",
    "    \n",
    "#     # Training loop\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f\"\\n== Epoch {epoch+1}/{num_epochs} ==\")\n",
    "#         print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "#         # Training phase\n",
    "#         train_loss, train_time = do_train(\n",
    "#             model,\n",
    "#             train_loader,\n",
    "#             optimizer,\n",
    "#             epoch,\n",
    "#             device,\n",
    "#             scheduler=(warmup_scheduler if epoch < warmup_epochs and warmup_epochs > 0 else None),\n",
    "#             max_grad_norm=1.0\n",
    "#         )\n",
    "        \n",
    "#         # Evaluation phase\n",
    "#         val_loss, val_acc, val_time = do_eval(model, val_loader, device)\n",
    "        \n",
    "#         # Step the scheduler\n",
    "#         if scheduler_type == \"plateau\":\n",
    "#             scheduler.step(val_loss)\n",
    "#         elif scheduler is not None and epoch >= warmup_epochs:\n",
    "#             scheduler.step()\n",
    "#         elif warmup_epochs > 0 and epoch < warmup_epochs:\n",
    "#             warmup_scheduler.step()\n",
    "        \n",
    "#         # Save checkpoint if it's the best model so far\n",
    "#         if val_acc > best_val_acc:\n",
    "#             best_val_acc = val_acc\n",
    "#             checkpoint_path = os.path.join(checkpoint_dir, f\"clip_best_model_epoch{epoch+1}.pt\")\n",
    "            \n",
    "#             # Save model state\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch + 1,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'val_acc': val_acc,\n",
    "#                 'val_loss': val_loss,\n",
    "#             }, checkpoint_path)\n",
    "            \n",
    "#             best_model_path = checkpoint_path\n",
    "#             print(f\"ðŸ”¥ New best model saved to {checkpoint_path}\")\n",
    "    \n",
    "#     print(f\"\\n== Training Complete ==\")\n",
    "#     print(f\"Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "#     print(f\"Best model saved at: {best_model_path}\")\n",
    "    \n",
    "#     return model, best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0520a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model, best_model_path = train_clip_model(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     num_epochs=10,\n",
    "#     learning_rate=2e-5,\n",
    "#     scheduler_type=\"cosine\",  # Try \"cosine\", \"linear\", or \"plateau\"\n",
    "#     warmup_epochs=1\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4014d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7fc86fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 1 - Train Loss: 2.1983 - Time: 318.81s\n",
      "ðŸ“Š Validation Loss: 1.9128 | val_Acc: 33.8668% |Top-1 Acc: 33.87% | Top-5 Acc: 79.88% | Time: 20.20s\n",
      "\n",
      "ðŸš€Epoch 1 Summary:\n",
      "âœ… Epoch 1 - Train Loss: 2.1983 - Time: 318.81s\n",
      "\n",
      "ðŸš€ Epoch 2/3\n",
      "âœ… Epoch 2 - Train Loss: 1.6627 - Time: 322.32s\n",
      "ðŸ“Š Validation Loss: 1.4745 | val_Acc: 47.9099% |Top-1 Acc: 47.91% | Top-5 Acc: 90.46% | Time: 20.22s\n",
      "\n",
      "ðŸš€Epoch 2 Summary:\n",
      "âœ… Epoch 2 - Train Loss: 1.6627 - Time: 322.32s\n",
      "\n",
      "ðŸš€ Epoch 3/3\n",
      "âœ… Epoch 3 - Train Loss: 1.2863 - Time: 322.71s\n",
      "ðŸ“Š Validation Loss: 1.1077 | val_Acc: 59.4709% |Top-1 Acc: 59.47% | Top-5 Acc: 95.85% | Time: 20.20s\n",
      "\n",
      "ðŸš€Epoch 3 Summary:\n",
      "âœ… Epoch 3 - Train Loss: 1.2863 - Time: 322.71s\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "EPOCHS=3\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nðŸš€ Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    avg_train_loss, train_time = do_train(model, train_loader, optimizer, epoch, device=\"cuda\")\n",
    "    avg_val_loss,val_acc ,val_acc_top1,val_acc_top5, val_time = do_eval(model, val_loader, device=\"cuda\")\n",
    "    print(f\"\\nðŸš€Epoch {epoch+1} Summary:\")\n",
    "    print(f\"âœ… Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f} - Time: {train_time:.2f}s\")\n",
    "    # val_acc = float(val_acc)\n",
    "    # val_acc_top1 = float(val_acc_top1)\n",
    "    # val_acc_top5 = float(val_acc_top5)\n",
    "    # val_time = float(val_time)\n",
    "\n",
    "\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2636e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b130bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Training loop\n",
    "# model.train()\n",
    "# for epoch in range(3):\n",
    "#     for batch in train_loader:\n",
    "#         batch = {k: v.cuda() for k, v in batch.items()}\n",
    "#         outputs = model(**batch, return_loss=True)\n",
    "#         loss = outputs.loss\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1} - Loss: {loss.item():.4f}\")\n",
    "# EPOCHS = 10\n",
    "# # Learning rate scheduler\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "# # Move model to GPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# # Training and evaluation loop\n",
    "# print(\"Starting training and evaluation loop...\")\n",
    "# print(f\"Using device: {device}\")\n",
    "# for epoch in range(EPOCHS):\n",
    "#     print(f\"\\nðŸš€ Epoch {epoch+1}/{EPOCHS}\")\n",
    "#     train_loss, train_time = do_train(model, train_loader, optimizer, lr_scheduler, device)\n",
    "#     # val_loss, val_acc, val_time = do_eval(model, val_loader, device)\n",
    "\n",
    "#     print(\"\\n=================================================================================\")\n",
    "#     print(f\"Epoch {epoch+1} Summary:\")\n",
    "#     print(f\"  Train Loss: {train_loss:.4f} | Train Time: {str(timedelta(seconds=int(train_time)))}\")\n",
    "#     # print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val Time: {str(timedelta(seconds=int(val_time)))}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eda932ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_624551/1221947276.py:11: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.title(\"CLIP Fine-Tuning Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(val_accuracies, label=\"Val Accuracy\")\n",
    "# plt.title(\"CLIP Val Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c225236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceeba65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61612858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plthj\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def plot_training_metrics(\n",
    "    train_losses, \n",
    "    val_losses, \n",
    "    val_accuracies=None, \n",
    "    learning_rates=None, \n",
    "    save_dir='plots'\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot training metrics including losses, accuracy, and learning rate.\n",
    "    \n",
    "    Args:\n",
    "        train_losses: List of training losses per epoch\n",
    "        val_losses: List of validation losses per epoch\n",
    "        val_accuracies: List of validation accuracies per epoch (optional)\n",
    "        learning_rates: List of learning rates per epoch (optional)\n",
    "        save_dir: Directory to save the plots\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate timestamp for unique filenames\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Set style for plots\n",
    "    plt.style.use('ggplot')\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.plot(epochs, train_losses, 'b-o', label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-o', label='Validation Loss')\n",
    "    \n",
    "    plt.title('CLIP Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save the loss plot\n",
    "    loss_plot_path = os.path.join(save_dir, f'clip_losses_{timestamp}.png')\n",
    "    plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Loss plot saved to {loss_plot_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot validation accuracy if provided\n",
    "    if val_accuracies:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, val_accuracies, 'g-o')\n",
    "        plt.title('CLIP Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Save the accuracy plot\n",
    "        acc_plot_path = os.path.join(save_dir, f'clip_accuracy_{timestamp}.png')\n",
    "        plt.savefig(acc_plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Accuracy plot saved to {acc_plot_path}\")\n",
    "        plt.close()\n",
    "    \n",
    "    # Plot learning rate if provided\n",
    "    if learning_rates:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, learning_rates, 'purple', marker='o')\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.yscale('log')  # Log scale often better for LR visualization\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Save the learning rate plot\n",
    "        lr_plot_path = os.path.join(save_dir, f'clip_lr_schedule_{timestamp}.png')\n",
    "        plt.savefig(lr_plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Learning rate plot saved to {lr_plot_path}\")\n",
    "        plt.close()\n",
    "    \n",
    "    # Combined plot with dual y-axis\n",
    "    if val_accuracies:\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        # Loss curves on left y-axis\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss', color='tab:blue')\n",
    "        ax1.plot(epochs, train_losses, 'b-o', label='Training Loss')\n",
    "        ax1.plot(epochs, val_losses, 'r-o', label='Validation Loss')\n",
    "        ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "        \n",
    "        # Accuracy curve on right y-axis\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_ylabel('Accuracy (%)', color='tab:green')\n",
    "        ax2.plot(epochs, val_accuracies, 'g-o', label='Validation Accuracy')\n",
    "        ax2.tick_params(axis='y', labelcolor='tab:green')\n",
    "        \n",
    "        # Add legend\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "        \n",
    "        plt.title('CLIP Training Metrics')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Save the combined plot\n",
    "        combined_plot_path = os.path.join(save_dir, f'clip_combined_metrics_{timestamp}.png')\n",
    "        plt.savefig(combined_plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Combined metrics plot saved to {combined_plot_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Updated training function to track and plot metrics\n",
    "def train_clip_model_with_plots(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=10,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    scheduler_type=\"cosine\",\n",
    "    warmup_epochs=1,\n",
    "    checkpoint_dir=\"checkpoints\",\n",
    "    plots_dir=\"plots\",\n",
    "    device=None\n",
    "):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Create scheduler based on type\n",
    "    scheduler = None\n",
    "    if scheduler_type == \"cosine\":\n",
    "        # Cosine decay from initial lr to near-zero\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, \n",
    "            T_max=num_epochs - warmup_epochs,\n",
    "            eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # Optional: Warmup scheduler for first few epochs\n",
    "        if warmup_epochs > 0:\n",
    "            warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "                optimizer, \n",
    "                start_factor=0.1, \n",
    "                end_factor=1.0, \n",
    "                total_iters=warmup_epochs\n",
    "            )\n",
    "            \n",
    "    elif scheduler_type == \"linear\":\n",
    "        # Linear decay\n",
    "        scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer,\n",
    "            start_factor=1.0,\n",
    "            end_factor=0.1,\n",
    "            total_iters=num_epochs\n",
    "        )\n",
    "        \n",
    "    elif scheduler_type == \"plateau\":\n",
    "        # Reduce LR when validation loss plateaus\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            verbose=True\n",
    "        )\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Metrics tracking\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    learning_rates = []\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_model_path = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rates.append(current_lr)\n",
    "        \n",
    "        print(f\"\\n== Epoch {epoch+1}/{num_epochs} ==\")\n",
    "        print(f\"Learning rate: {current_lr:.2e}\")\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss, train_time = do_train(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            device,\n",
    "            scheduler=(warmup_scheduler if epoch < warmup_epochs and warmup_epochs > 0 else None),\n",
    "            max_grad_norm=1.0\n",
    "        )\n",
    "        \n",
    "        # Evaluation phase\n",
    "        val_loss, val_acc, val_time = do_eval(model, val_loader, device)\n",
    "        \n",
    "        # Record metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc * 100)  # Convert to percentage\n",
    "        \n",
    "        # Step the scheduler\n",
    "        if scheduler_type == \"plateau\":\n",
    "            scheduler.step(val_loss)\n",
    "        elif scheduler is not None and epoch >= warmup_epochs:\n",
    "            scheduler.step()\n",
    "        elif warmup_epochs > 0 and epoch < warmup_epochs:\n",
    "            warmup_scheduler.step()\n",
    "        \n",
    "        # Save checkpoint if it's the best model so far\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"clip_best_model_epoch{epoch+1}.pt\")\n",
    "            \n",
    "            # Save model state\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'val_accuracies': val_accuracies,\n",
    "                'learning_rates': learning_rates\n",
    "            }, checkpoint_path)\n",
    "            \n",
    "            best_model_path = checkpoint_path\n",
    "            print(f\"ðŸ”¥ New best model saved to {checkpoint_path}\")\n",
    "        \n",
    "        # Plot metrics every few epochs or at the end\n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            plot_training_metrics(\n",
    "                train_losses, \n",
    "                val_losses, \n",
    "                val_accuracies, \n",
    "                learning_rates,\n",
    "                save_dir=plots_dir\n",
    "            )\n",
    "    \n",
    "    # Final plots\n",
    "    plot_training_metrics(\n",
    "        train_losses, \n",
    "        val_losses, \n",
    "        val_accuracies, \n",
    "        learning_rates,\n",
    "        save_dir=plots_dir\n",
    "    )\n",
    "    \n",
    "    # Save training history to CSV\n",
    "    history_df = pd.DataFrame({\n",
    "        'epoch': list(range(1, num_epochs + 1)),\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'val_accuracy': val_accuracies,\n",
    "        'learning_rate': learning_rates\n",
    "    })\n",
    "    \n",
    "    history_path = os.path.join(plots_dir, f'training_history_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv')\n",
    "    history_df.to_csv(history_path, index=False)\n",
    "    print(f\"Training history saved to {history_path}\")\n",
    "    \n",
    "    print(f\"\\n== Training Complete ==\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "    print(f\"Best model saved at: {best_model_path}\")\n",
    "    \n",
    "    return model, best_model_path, history_df\n",
    "\n",
    "\n",
    "# Example usage with the do_train and do_eval functions from previous code\n",
    "def do_train(model, train_loader, optimizer, epoch, device, scheduler=None, max_grad_norm=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch, return_loss=True)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        if max_grad_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Report progress for long epochs\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    print(f\"âœ… Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f} - Time: {train_time:.2f}s\")\n",
    "    return avg_train_loss, train_time\n",
    "\n",
    "@torch.no_grad()\n",
    "def do_eval(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch in val_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Get loss\n",
    "        outputs = model(**batch, return_loss=True)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Get image and text features\n",
    "        image_embeds = model.get_image_features(pixel_values=batch['pixel_values'])\n",
    "        text_embeds = model.get_text_features(\n",
    "            input_ids=batch['input_ids'], \n",
    "            attention_mask=batch['attention_mask']\n",
    "        )\n",
    "        \n",
    "        # Normalize\n",
    "        image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "        text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Compute similarity\n",
    "        logits = torch.matmul(image_embeds, text_embeds.T) * model.logit_scale.exp()\n",
    "        \n",
    "        # Prediction (diagonal elements should be highest)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        labels = torch.arange(len(preds)).to(device)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    val_time = time.time() - start_time\n",
    "    avg_val_loss = total_loss / len(val_loader)\n",
    "    val_acc = len([i for i, (p, l) in enumerate(zip(all_preds, all_labels)) if p == l]) / len(all_preds)\n",
    "    \n",
    "    print(f\"ðŸ“Š Validation Loss: {avg_val_loss:.4f} - Val_Accuracy: {val_acc*100:.2f}% - Time: {val_time:.2f}s\")\n",
    "    return avg_val_loss, val_acc, val_time\n",
    "\n",
    "\n",
    "# Example of how to use the plotting functionality\n",
    "if __name__ == \"__main__\":\n",
    "    from transformers import CLIPModel, CLIPProcessor\n",
    "    \n",
    "    # Load pre-trained CLIP model and processor\n",
    "    model_name = \"openai/clip-vit-base-patch32\"\n",
    "    model = CLIPModel.from_pretrained(model_name)\n",
    "    processor = CLIPProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    # Prepare your datasets and dataloaders here\n",
    "    # train_dataset = ...\n",
    "    # val_dataset = ...\n",
    "    # train_loader = ...\n",
    "    # val_loader = ...\n",
    "    \n",
    "    # Train the model with plotting\n",
    "    trained_model, best_model_path, history_df = train_clip_model_with_plots(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=20,\n",
    "        learning_rate=2e-5,\n",
    "        scheduler_type=\"cosine\",\n",
    "        warmup_epochs=2,\n",
    "        plots_dir=\"clip_training_plots\"\n",
    "    )\n",
    "    \n",
    "    # If you want to plot metrics from a saved checkpoint:\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load('path/to/checkpoint.pt')\n",
    "    plot_training_metrics(\n",
    "        checkpoint['train_losses'],\n",
    "        checkpoint['val_losses'],\n",
    "        checkpoint['val_accuracies'],\n",
    "        checkpoint['learning_rates'],\n",
    "        save_dir='restored_plots'\n",
    "    )\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
