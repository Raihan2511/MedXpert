{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46326659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3fc8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading the dataset...\")\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"itsanmolgupta/mimic-cxr-dataset\")\n",
    "# dataset['train'], dataset['validation'], dataset['test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7350a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available splits\n",
    "print(\"Available splits:\", list(dataset.keys()))\n",
    "print(\"Number of samples in each split:\")\n",
    "for split in dataset.keys():\n",
    "    print(f\"{split}: {len(dataset[split])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba3dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one sample\n",
    "sample = dataset['train'][1]\n",
    "\n",
    "print(\"\\n--- Sample Data ---\")\n",
    "pprint(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7585f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.__sizeof__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b73cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = sample['image']\n",
    "print(\"Image size:\", sample_image.size)\n",
    "print(\"Image mode:\", sample_image.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4724338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming sample_data contains your data with the image\n",
    "def display_sample(sample_data):\n",
    "    # Extract components\n",
    "    image = sample_data['image']\n",
    "    findings = sample_data['findings']\n",
    "    impression = sample_data['impression']\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(4,6))\n",
    "    \n",
    "    # Display image\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title('Chest X-ray')\n",
    "    \n",
    "    # Display text\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.axis('off')\n",
    "    plt.text(0, 0.8, f\"Findings: {findings}\", wrap=True, fontsize=12)\n",
    "    plt.text(0, 0.2, f\"Impression: {impression}\", wrap=True, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your sample data\n",
    "count = 0\n",
    "for sample in dataset['train']:\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break\n",
    "    display_sample(sample)  # Replace with your actual variable name\n",
    "# Replace with your actual variable name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92499a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Define sampling fraction\n",
    "fraction = 0.001\n",
    "total_examples = len(dataset[\"train\"])\n",
    "print(\"total_examples:\",total_examples)\n",
    "# Define the number of examples to sample\n",
    "subset_size = int(total_examples * fraction)\n",
    "print(\"subset_size:\",subset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9532ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample subset indices from the full dataset\n",
    "sampled_indices = random.sample(range(total_examples), subset_size)\n",
    "\n",
    "# Shuffle the subset for randomness\n",
    "random.shuffle(sampled_indices)\n",
    "\n",
    "# Split into train and validation from the sampled subset\n",
    "train_cutoff = int(subset_size * 0.8)\n",
    "train_indices = sampled_indices[:train_cutoff]\n",
    "val_indices = sampled_indices[train_cutoff:]\n",
    "\n",
    "# Apply indices to the original full dataset\n",
    "dataset[\"validation\"] = dataset[\"train\"].select(val_indices)\n",
    "dataset[\"train\"] = dataset[\"train\"].select(train_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97192d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of samples in the new train set:\", len(dataset[\"train\"]))\n",
    "print(\"Number of samples in the new validation set:\", len(dataset[\"validation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb0afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92676da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MIMICCLIPDataset(Dataset):\n",
    "#     def __init__(self, hf_dataset, processor):\n",
    "#         self.dataset = hf_dataset\n",
    "#         self.processor = processor\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = self.dataset[idx]\n",
    "#         image = item[\"image\"]  # already a PIL.Image from dataset\n",
    "#         text = item.get(\"impression\") or item.get(\"findings\")\n",
    "\n",
    "#         inputs = self.processor(\n",
    "#             text=text,\n",
    "#             images=image,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=True\n",
    "#         )\n",
    "#         return {k: v.squeeze(0) for k, v in inputs.items()}, None\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7e3070",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICCLIPDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, processor, max_length=77):\n",
    "        self.dataset = hf_dataset\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length  # CLIP default context length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item[\"image\"]  # already a PIL.Image from dataset\n",
    "        \n",
    "        # Preferentially use impression if available, otherwise findings\n",
    "        text = item.get(\"impression\") or item.get(\"findings\")\n",
    "        \n",
    "        # Process with CLIP processor \n",
    "        inputs = self.processor(\n",
    "            text=text,\n",
    "            images=image,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension added by processor\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Example of how to create and use the DataLoader with your dataset\n",
    "'''\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = MIMICCLIPDataset(train_hf_dataset, processor)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16, \n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Example training loop\n",
    "for batch, _ in train_loader:\n",
    "    # batch is now a dict with keys like 'input_ids', 'attention_mask', 'pixel_values'\n",
    "    # that can be passed directly to your CLIP model\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    # ... rest of training code\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c318dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/clip/train.py\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "def do_train(model, train_dl, optimizer, lr_scheduler, device):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for bid, batch in enumerate(train_dl):\n",
    "        batch_start = time.time()\n",
    "        if bid % 100 == 0:\n",
    "            print(\"...{:d} training steps complete\".format(bid))\n",
    "\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch, return_loss=True)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        train_loss += loss.detach().cpu().numpy()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if bid % 100 == 0 and bid > 0:\n",
    "            batch_time = time.time() - batch_start\n",
    "            eta = batch_time * (len(train_dl) - bid)\n",
    "            print(f\"    Batch time: {batch_time:.2f}s | ETA: {str(timedelta(seconds=int(eta)))}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    avg_time_per_batch = total_time / len(train_dl)\n",
    "    print(f\"...{bid} training steps COMPLETE in {str(timedelta(seconds=int(total_time)))}\")\n",
    "    print(f\"Average time per batch: {avg_time_per_batch:.2f}s\")\n",
    "    \n",
    "    return train_loss, total_time\n",
    "\n",
    "#     return val_loss, val_acc, total_time\n",
    "def do_eval(model, eval_dl, device):\n",
    "    model.eval()\n",
    "    val_loss, val_acc, num_examples = 0, 0, 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Add debugging to check if dataloader is empty\n",
    "    print(f\"Validation dataloader contains {len(eval_dl)} batches\")\n",
    "    \n",
    "    for bid, batch in enumerate(eval_dl):\n",
    "        # Print every batch during validation for debugging\n",
    "        print(f\"Validating batch {bid+1}/{len(eval_dl)}\")\n",
    "        \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch, return_loss=True)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        val_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        probs = logits_per_image.softmax(dim=1)\n",
    "        predictions = torch.argmax(probs, dim=-1)\n",
    "        labels = torch.arange(len(predictions)).to(device)\n",
    "\n",
    "        accuracy = torch.sum(predictions == labels)\n",
    "        num_examples += len(predictions)\n",
    "        val_acc += accuracy\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Avoid division by zero if no examples were processed\n",
    "    if num_examples > 0:\n",
    "        val_acc = val_acc.detach().cpu().numpy() / num_examples\n",
    "    else:\n",
    "        val_acc = 0.0\n",
    "        print(\"WARNING: No examples were processed during validation!\")\n",
    "    \n",
    "    print(f\"Validation complete: Processed {num_examples} examples in {len(eval_dl)} batches\")\n",
    "    \n",
    "    return val_loss, val_acc, total_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96af900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of setting up training\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Setup dataloaders\n",
    "train_ds = MIMICCLIPDataset(dataset[\"train\"], processor)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_ds = MIMICCLIPDataset(dataset[\"validation\"], processor)\n",
    "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=4)\n",
    "# Check validation dataset size\n",
    "print(f\"Validation dataset size: {len(val_ds)} examples\")\n",
    "print(f\"Validation batch size: {val_loader.batch_size}\")\n",
    "print(f\"Expected number of batches: {len(val_ds) // val_loader.batch_size + (1 if len(val_ds) % val_loader.batch_size > 0 else 0)}\")\n",
    "\n",
    "# Setup model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model.to(device)\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = len(train_loader) * 3  # Assuming 3 epochs\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nðŸš€ Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss,train_time = do_train(model, train_loader, optimizer, lr_scheduler, device)\n",
    "    val_loss, val_acc ,val_time= do_eval(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Time: {str(timedelta(seconds=int(train_time)))}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val Time: {str(timedelta(seconds=int(val_time)))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
