# ğŸ˜º MedXpert: Medical Visual QA & Diagnosis Assistant

**MedXpert** is a full-stack AI system designed to support medical diagnosis using visual-language retrieval and automated report generation. It combines a **fine-tuned CLIP model** for image-text alignment, **BLIP** for medical image captioning, and **Gemini AI (LLM)** for structured diagnosis generation.

---

## ğŸ§  Goal

Provide intelligent diagnostic assistance from:

* ğŸ–¼ï¸ A chest X-ray (or any medical image), or
* ğŸ’¬ A free-text query (e.g., â€œSigns of pneumonia?â€)

And return:

* Top-K matching cases (images or text)
* Interpretable image captions
* Structured diagnostic summaries

---

## ğŸ¯ Why Fine-Tune CLIP?

CLIP (Contrastive Languageâ€“Image Pretraining) aligns images and text in a shared space. However, **zero-shot CLIP** trained on internet data lacks medical context.

To solve this, we **fine-tuned CLIP** using a medical dataset to better understand domain-specific terms like *cardiomegaly* or *infiltrate*.

---

## ğŸ©º Fine-Tuning with MIMIC-CXR

We used the **MIMIC-CXR** dataset of X-rays and clinical reports.

**ğŸ“¦ Dataset Summary:**

* **377k+** X-rays
* **227k+** Reports
* **14** diagnostic classes
* **Split:** 70/15/15 (Train/Val/Test)

**ğŸ“Š Samples Used:**

* Train: 24,499
* Val/Test: 3,062 each
* Features: `image`, `findings`, `impression`
* Fallback: Use findings if impression is missing

---

## âš™ï¸ Training Setup

| Setting        | Value                          |
| -------------- | ------------------------------ |
| Model          | `openai/clip-vit-base-patch32` |
| Batch size     | 16                             |
| Epochs         | 3                              |
| Learning rate  | 5e-6                           |
| Optimizer      | AdamW                          |
| Save path      | `models/clip/fine_tuned/`      |
| Embedding path | `data/embeddings/`             |

**Training Logic:**

* Contrastive loss pulls image-report pairs together in embedding space
* Negative samples pushed apart using cosine similarity
* Visual encoderâ€™s early layers frozen for general feature retention
* Best checkpoint chosen by **image-to-text accuracy**

---

## ğŸ“ˆ Fine-Tuning Gains

| Metric     | Zero-Shot | Fine-Tuned |
| ---------- | --------- | ---------- |
| Recall\@1  | 12.4%     | **34.7%**  |
| Recall\@5  | 28.1%     | **61.2%**  |
| Recall\@10 | 41.5%     | **75.9%**  |

---

## ğŸ§© System Pipeline

```mermaid
flowchart TD
    U[User Input<br>(X-ray or Query)] -->|Embed| CLIP[Fine-Tuned CLIP Encoder]
    CLIP --> RETRIEVE[Similarity Search<br>(Top-K Retrieval)]
    U --> BLIP[BLIP Caption Generator]
    RETRIEVE --> LLM[Gemini AI]
    BLIP --> LLM
    LLM --> REPORT[Structured Diagnosis Report]
    REPORT --> UI[Streamlit Frontend]
```

---

## ğŸ’¡ Core Components

### ğŸ” CLIP: Visual-Text Embedding

* Embeds medical images and reports into a joint space for similarity comparison

### ğŸ§  BLIP: Image Captioning

* Generates human-readable descriptions from X-rays
* Helps LLMs understand image context without domain-specific tuning

### ğŸ“ Gemini AI (LLM): Report Generation

* Ingests BLIP captions and retrieved image-text matches
* Outputs a structured diagnosis including likely findings, severity, and suggestions

---

## ğŸ§  CLIP Architecture & Contrastive Learning

### ğŸ–¼ï¸ CLIP Architecture

![CLIP Architecture](f47943d0-94ef-4cd7-9e2d-54b3d20e90d2.png)

### ğŸ”„ What is Contrastive Learning?

In contrastive learning, models are trained to bring similar inputs closer in embedding space and push dissimilar ones apart.

**Example:**

* A positive pair might be an X-ray and its correct report.
* A negative pair would be an X-ray and an unrelated report.

This technique improves the model's ability to distinguish relevant matches.

### ğŸ§± Component Summary

* **Image Encoder:** Extracts visual features and maps image to vector.
* **Text Encoder:** Encodes textual description to vector.
* **Shared Embedding Space:** Enables direct comparison of image and text.
* **Contrastive Loss:** Aligns correct image-text pairs while separating mismatched ones.

---

## ğŸš€ Features

* ğŸ”„ **Imageâ€“Text Search** (bidirectional)
* ğŸ§  **Human-Like Captions** via BLIP
* ğŸ“„ **AI Diagnosis Summaries** via Gemini LLM
* ğŸ–¥ï¸ **Streamlit UI** for clinicians
* ğŸ™ï¸ **Upcoming:** Voice input + ICD-10 tagging

---

## ğŸ§ª Try It Yourself

### 1ï¸âƒ£ Setup

```bash
git clone https://github.com/yourname/medxpert.git
cd medxpert
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2ï¸âƒ£ Train + Embed

```bash
python main.py
```

> Fine-tunes CLIP and generates embeddings

### 3ï¸âƒ£ Retrieval & Evaluation

```bash
python scripts/run_search_engine.py
python scripts/inference_plot.py
python scripts/inference_to_csv.py
python scripts/eval_retrieval.py
```

### 4ï¸âƒ£ Full Inference Pipeline

```bash
python scripts/run_inference_pipeline.py
```

> Uses `dummy_llm()` â€” replace with Gemini API for full functionality

---

## ğŸ–¼ï¸ Streamlit Frontend

```bash
streamlit run app/app.py
```

**UI Capabilities:**

* Upload X-ray or enter a query
* Visualize top-K matches
* View BLIP captions
* Generate final diagnosis report

---

## ğŸ“ Project Structure

```
medxpert/
â”œâ”€â”€ app/                        # Streamlit UI
â”œâ”€â”€ data/                       # Dataset & embeddings
â”œâ”€â”€ models/                     # Fine-tuned CLIP
â”œâ”€â”€ scripts/                    # Utility scripts
â”œâ”€â”€ llm_report_generation.py    # LLM integration
â”œâ”€â”€ main.py                     # CLIP training
â”œâ”€â”€ dev.sh                      # One-shot runner
â””â”€â”€ requirements.txt
```

---

## ğŸ“¬ Get in Touch

Have suggestions or want to collaborate? Open an issue on GitHub.

> âš ï¸ **Note:** MedXpert is a research tool â€” **not for clinical use.**
